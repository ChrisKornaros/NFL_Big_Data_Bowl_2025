{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database connection\n",
    "con = duckdb.connect(\".config/nfl.duckdb\")\n",
    "#con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframes with DuckDB, plays and player_play both have 50 columns, more ideal for a broad random forest\n",
    "X = con.sql(\"\"\"\n",
    "    SELECT quarter, down, yardsToGo, yardlineNumber, preSnapHomeScore, preSnapVisitorScore,\n",
    "    playNullifiedByPenalty, absoluteYardlineNumber, preSnapHomeTeamWinProbability, preSnapVisitorTeamWinProbability, expectedPoints,\n",
    "    passResult_complete, passResult_incomplete, passResult_sack, passResult_interception, passResult_scramble, passLength, targetX, targetY,\n",
    "    playAction, passTippedAtLine, unblockedPressure, qbSpike, qbKneel, qbSneak, penaltyYards, prePenaltyYardsGained, \n",
    "    homeTeamWinProbabilityAdded, visitorTeamWinProbilityAdded, expectedPointsAdded, isDropback, timeToThrow, timeInTackleBox, timeToSack,\n",
    "    dropbackDistance, pff_runPassOption, playClockAtSnap, pff_manZone, pff_runConceptPrimary_num, pff_passCoverage_num, pff_runConceptSecondary_num\n",
    "FROM silver.plays_rf\n",
    "\"\"\").df()\n",
    "y = np.array(con.sql(\"\"\"\n",
    "    SELECT yardsGained\n",
    "    FROM silver.plays_rf\n",
    "\"\"\").df()).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64) \n",
      " (16124, 41) \n",
      " (16124,)\n"
     ]
    }
   ],
   "source": [
    "# Having issues with NA values, the below code does a simple count using pandas, will then go back and change the query\n",
    "# As of writing this, the issue is solved; however, the dbt model for this is far from efficient\n",
    "na_counts = (X == 'NA').sum()\n",
    "\n",
    "# Optionally, filter only columns with 'NA' values for easier review\n",
    "na_counts_filtered = na_counts[na_counts > 0]\n",
    "print(na_counts_filtered, \"\\n\", X.shape, \"\\n\", y.shape) # playClockAtSnap has only 1 NA value, will just drop that row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and split the data\n",
    "rf = RandomForestRegressor(warm_start=True)\n",
    "\n",
    "selector = RFE(rf, n_features_to_select=10, step=1)\n",
    "X_selected = selector.fit_transform(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['yardlineNumber', 'absoluteYardlineNumber',\n",
      "       'preSnapHomeTeamWinProbability', 'expectedPoints',\n",
      "       'passResult_scramble', 'penaltyYards', 'prePenaltyYardsGained',\n",
      "       'homeTeamWinProbabilityAdded', 'visitorTeamWinProbilityAdded',\n",
      "       'expectedPointsAdded'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Begin Interpretation, first with feature importance\n",
    "selected_features = X.columns[selector.support_]\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.832202573643411\n",
      "R^2 Score: 0.9759363607520203\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate scores\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with the GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=4)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Wrap a progress bar for longer Grid Searches\n",
    "with tqdm(\n",
    "    total=len(param_grid['n_estimators']) * len(param_grid['max_depth']) *\n",
    "          len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf']),\n",
    "    desc=\"GridSearch Progress\") as pbar:\n",
    "    def callback(*args, **kwargs):\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Add the callback to the grid search\n",
    "    grid_search.fit(X, y, callback=callback)\n",
    "\n",
    "#print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with the Cross Validation Score\n",
    "cv_scores = cross_val_score(rf, X_selected, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Cross-validated MSE: {-cv_scores.mean()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
